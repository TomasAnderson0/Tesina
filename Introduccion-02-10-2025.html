<!DOCTYPE html>
<html lang="en"><head>
<script src="Introduccion-02-10-2025_files/libs/clipboard/clipboard.min.js"></script>
<script src="Introduccion-02-10-2025_files/libs/quarto-html/tabby.min.js"></script>
<script src="Introduccion-02-10-2025_files/libs/quarto-html/popper.min.js"></script>
<script src="Introduccion-02-10-2025_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="Introduccion-02-10-2025_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Introduccion-02-10-2025_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="Introduccion-02-10-2025_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="Introduccion-02-10-2025_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.551">

  <title>Inserte titulo aqui</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="Introduccion-02-10-2025_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="Introduccion-02-10-2025_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="Introduccion-02-10-2025_files/libs/revealjs/dist/theme/quarto.css">
  <link href="Introduccion-02-10-2025_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="Introduccion-02-10-2025_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="Introduccion-02-10-2025_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="Introduccion-02-10-2025_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="https://static.vecteezy.com/system/resources/previews/006/413/049/non_2x/green-gradient-pastle-soft-beautiful-abstract-background-you-can-use-this-background-for-your-content-like-as-technology-video-gaming-promotion-card-banner-sports-presentation-website-etc-vector.jpg" class="quarto-title-block center">
  <h1 class="title">Inserte titulo aqui</h1>
  <p class="subtitle">Inserte subtitulo aqui</p>

<div class="quarto-title-authors">
</div>

</section>
<section id="que-es-el-reinforcement-learning" class="slide level2">
<h2>¿Que es el <em>Reinforcement Learning</em>?</h2>
<div class="rows">
<div class="row" height="50%">
<p>El aprendizaje por refuerzo (RL) es un tipo de aprendizaje automático en el que un agente aprende, mediante prueba y error, a tomar decisiones en un entorno con el objetivo de maximizar una señal de recompensa numérica acumulada.</p>
</div>
<div class="row" height="50%">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://media.geeksforgeeks.org/wp-content/uploads/20220214110501/ImagefromiOS1-660x296.jpg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="elementos-del-rl" class="slide level2">
<h2>Elementos del RL</h2>
<div style="font-size: 87%;">
<ul>
<li>Agente: Entidad que toma decisiones e interactúa con el entorno para lograr un objetivo.</li>
<li>Entorno: Todo lo externo al agente, con lo que este interactúa y que responde a sus acciones.</li>
<li>Estado(S): Representación de la información relevante del entorno que el agente puede observar en un momento dado.</li>
<li>Recompensa(R): Señal numérica proporcionada por el entorno que indica al agente qué tan buena fue una acción.</li>
<li>Acción(A): Decisión o movimiento que el agente realiza para influir en el entorno.</li>
</ul>
</div>
</section>
<section id="proceso-de-decisión-de-markovmdp" class="slide level2">
<h2>Proceso de decisión de Markov(MDP)</h2>
<div class="rows">
<div class="row" height="30%">
<div class="columns">
<div class="column" style="width:50%;">
<div style="font-size: 70%;">
<p>En un estado dado del entorno, el agente debe seleccionar una acción. Al ejecutar dicha acción, el entorno responde proporcionando un nuevo estado y una recompensa numérica asociada a esa transición. Repitiendo este proceso de forma cíclica —ya sea hasta alcanzar un objetivo o hasta que finalice la tarea (estado terminal)— se genera una secuencia o <em>trayectoria</em> del tipo:</p>
</div>
</div><div class="column" style="width:50%;">
<p><img data-src="https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/8f/5e/reinforcement-learning-figure-1.png" style="width:1000.0%"></p>
</div>
</div>
</div>
<div class="row" height="70%">
<div style="font-size: 70%;">
<p><span class="math display">\[S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,...\]</span> Esta secuencia es conocida como un Proceso de Decisión de Markov (MDP)</p>
</div>
</div>
</div>
</section>
<section id="mdp-finito" class="slide level2">
<h2>MDP Finito</h2>
<div style="font-size: 70%;">
<p>Si el conjunto de estados, acciones y recompensas(<span class="math inline">\(\boldsymbol{S}\)</span>, <span class="math inline">\(\boldsymbol{A}\)</span> y <span class="math inline">\(\boldsymbol{R}\)</span>) tienen un numero finito de elementos, entonces se dice que el MDP es finito. En este caso, la probabilidad de obtener una recompensa <span class="math inline">\(r \in \boldsymbol{R}\)</span> y un estado <span class="math inline">\(s' \in \boldsymbol{S}\)</span> en un tiempo t , dado un estado anterior s y una acción tomada a, se define como: <span class="math display">\[p(s', r | s, a) = Pr(S_t = s' , R_t | S_{t-1}=s,A_{t-1}=a )\]</span> La función p describe la dinámica del MDP. Si esta función caracteriza completamente el entorno —es decir, si la probabilidad de cada posible par de estado y recompensa futuros depende únicamente del estado y la acción anteriores—, entonces se dice que el proceso posee la <strong>propiedad de Markov</strong>.</p>
</div>
</section>
<section id="recompensa" class="slide level2">
<h2>Recompensa</h2>
<div style="font-size: 70%;">
<p>Para alcanzar su objetivo, el agente recibe en cada paso una recompensa, que es simplemente un número real <span class="math inline">\(R_t \in \mathbb{R}\)</span>. Informalmente, el agente debe maximizar el total de recompensas que recibe a lo largo del tiempo, es decir, debe priorizar la suma acumulativa de recompensas a largo plazo. En otras palabras, se busca maximizar el retorno esperado, denotado como <span class="math inline">\(G_t\)</span>, el cual es una función de la secuencia de recompensas futuras. El caso más simple de retorno es la suma de todas las recompensas futuras:</p>
<p><span class="math display">\[G_t = R_{t+1}+R_{t+2}+R_{t+3}+... =   \sum_{k=0}^\infty R_{t+k+1} \ \ \ \ \ \ \ \ \ \ \ \ (1.1)\]</span></p>
<p>Si existe una noción de un paso final, y la interacción entre el agente y el entorno puede dividirse en subsecuencias con un comienzo y un fin definidos —llamadas <em>episodios</em>—, entonces la tarea se denomina <em>episódica</em>. En cambio, si la interacción no se divide en episodios identificables y continúa indefinidamente, la tarea se considera <em>continua</em>.</p>
</div>
</section>
<section id="descuento" class="slide level2">
<h2>Descuento</h2>
<div style="font-size: 70%;">
<p>Si la tarea es continua, el retorno <span class="math inline">\(G_t\)</span> puede ser infinito. Para evitarlo, se introduce un factor de descuento <span class="math inline">\(\gamma \in [0,1]\)</span>.</p>
<p><span class="math display">\[G_t = R_{t+1}+\gamma R_{t+2}+ \gamma^2R_{t+3}+... = \sum_{k=0}^\infty \gamma^kR_{t+k+1} \ \ \ \ \ \ \ \ \ \ \ \ (1.2) \]</span> Mientras <span class="math inline">\(\gamma&lt;1\)</span> la suma infinita en (1.2) es finita siempre que las recompensas <span class="math inline">\(R_t\)</span> esten acotadas. Cuando <span class="math inline">\(\gamma = 0\)</span>, se dice que el agente es <em>miope</em>, pues solo maximiza las recompensas inmediatas. Cuanto más se acerque <span class="math inline">\(\gamma\)</span> a 1, mayor será la importancia que el retorno otorgue a las recompensas futuras.</p>
<span class="math display">\[\begin{matrix}
\begin{matrix}
G_t = R_{t+1}+\gamma R_{t+2}+ \gamma^2R_{t+3}+... = \\
= R_{t+1}+\gamma (R_{t+2}+ \gamma R_{t+3}+...) =  \hfill \\ = R_{t+1} + \gamma G_{t+1} \hfill
\end{matrix} &amp; \hfill (1.3)
\end{matrix}\]</span>
</div>
</section>
<section id="funcion-de-valor-y-política" class="slide level2">
<h2>Funcion de valor y política</h2>
<div style="font-size: 70%;">
<p>Para el agente saber que tan bien una accion en cierto estado se necesita una funcion que estime las recompensas futuras promedio que la accion traera. Esto se logra con la llamada funcion de valor, que depende de la accion que el agente tome, es decir, depende de su politica. La politica es el mapeo de los estados a la probabilidad de seleccionar cada posible accion. Si el agente sigue una politica <span class="math inline">\(\pi\)</span> al tiempo t, entonces <span class="math inline">\(\pi(a|s)\)</span> es la probabilidad de <span class="math inline">\(A_t=a\)</span> si <span class="math inline">\(S_t=s\)</span>. La funcion de valor siguiendo la politica , <span class="math inline">\(v_\pi(s)\)</span>, es el retorno esperado de seguir una politica cuando se empieza en el estado s. Llamamos a la funcion <span class="math inline">\(v_\pi\)</span> funcion estado-valor de la politica <span class="math inline">\(\pi\)</span>.</p>
<p><span class="math display">\[v_\pi(s) = E_\pi[G_t|S_T=s] = E_\pi[\sum_{k=0}^\infty \gamma^kR_{t+k+1}| S_t=s] \ \ \ \  \forall s \in \boldsymbol{S} \]</span></p>
</div>
</section>
<section id="ecuación-de-bellman" class="slide level2">
<h2>Ecuación de Bellman</h2>
<div style="font-size: 70%;">
<div class="rows">
<div class="row" height="30%">
<p>Una propiedad fundamental de las funciones de valores es que satisfacen una relacion recursiva. Para cualquier polica <span class="math inline">\(\pi\)</span> y estado s, se cumple una condición de consistencia entre el valor de dicho estado y los valores de sus estados sucesores.</p>
</div>
<div class="row" height="70%">
<div class="columns">
<div class="column" style="width:70%;">
<p><span class="math inline">\(v_\pi(s) = E_\pi[G_t|S_T=s] \  \ \  por \ (1.3)\)</span> </p>
<p><span class="math inline">\(= E_\pi[R_{t+1}+\gamma G_{t+1}|S_T=s] \ \ \ (1.4)\)</span> </p>
<p><span class="math inline">\(= \sum_{a} \pi(a|s)\sum_{s'}\sum_{r}p(s',r|s,a)[r+\gamma E_\pi[G_{t+1}|S_{t+1}=s']]\)</span></p>
<p><span class="math inline">\(= \sum_{a} \pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')] \ \  \forall s \in \boldsymbol{S} \ \ (1.5)\)</span></p>
<p>La ecuación (1.5), conocida como <em>ecuación de Bellman</em>, expresa precisamente esta relación entre un estado y sus sucesores.</p>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-right">
<figure>
<p><img data-src="https://miro.medium.com/v2/resize:fit:612/1*NwP8EPGMmu8UuE5bycYRUQ.png" class="quarto-figure quarto-figure-right"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="optimalidad" class="slide level2">
<h2>Optimalidad</h2>
<div style="font-size: 70%;">
<p>De manera análoga, se define el valor de tomar una acción a en un estado s bajo una politica <span class="math inline">\(\pi\)</span> como el retorno esperado al comenzar en s, ejecutar la acción a y continuar siguiendo dicha política:</p>
<p><span class="math inline">\(q_\pi(s,a) = E_\pi[G_t|S_T=s,A_T =a] = E_\pi[\sum_{k=0}^\infty \gamma^kR_{t+k+1}| S_t=s,A_T =a]\)</span></p>
<p>Equivalentemente, puede expresarse como:</p>
<p><span class="math inline">\(q_\pi(s,a) = E_\pi[R_{t+1}+\gamma v(S_{t+1})|S_T=s,A_T=a]\)</span></p>
<p>A esta función se la denomina función de acción-valor de la política <span class="math inline">\(\pi\)</span>.</p>
<p>El objetivo del aprendizaje por refuerzo es encontrar una política que maximice las recompensas futuras esperadas, es decir, que maximice, es decir, que maximice <span class="math inline">\(v_\pi(s)\)</span>. Decimos que una politica <span class="math inline">\(\pi'\)</span> es mejor que otra <span class="math inline">\(\pi \Leftrightarrow v_{\pi'}(s)&gt;v_\pi(s) \ \forall s\in \boldsymbol{S}\)</span></p>
</div>
</section>
<section id="explotación-y-exploración" class="slide level2">
<h2>Explotación y exploración</h2>
<div style="font-size: 70%;">
<p>El agente no conoce de antemano cuál es la política óptima, por lo que tampoco puede saber si su política actual es la mejor posible. La búsqueda de una mejor política implica, en ocasiones, ejecutar acciones con menor recompensa esperada en el corto plazo, con la esperanza de descubrir alternativas que resulten más beneficiosas a largo plazo. Este dilema se lo conoce como el <strong>problema de explotacion vs exploracion</strong>.</p>
<p>Una estrategia simple para abordarlo es el método <span class="math inline">\(\epsilon\)</span>-greedy. Bajo este esquema, el agente selecciona una acción al azar con probabilidad <span class="math inline">\(\epsilon\)</span>, lo que le permite explorar nuevas posibilidades, y con probabilidad <span class="math inline">\(1-\epsilon\)</span> elige la mejor acción conocida hasta el momento, es decir, explota su conocimiento actual. Este tipo de estrategias se conocen como <strong>estocasticas</strong>, ya que asigna probabilidades estrictamente positivas a todas las acciones posibles en un estado dado. En contraste, una política <strong>determinista</strong> elige siempre la misma acción en cada estado, sin incorporar exploración.</p>
</div>
</section>
<section id="monte-carlo" class="slide level2">
<h2>Monte Carlo</h2>
<div style="font-size: 70%;">
<p>Una forma sencilla de estimar el valor de un estado es mediante el promedio de las recompensas obtenidas tras haber visitado dicho estado. A medida que aumenta la cantidad de visitas, este promedio converge al valor esperado. Este metodo se conoce como el metodo de <strong>Monte Carlo(MC)</strong>.</p>
<p>La principal ventaja de MC es que no requiere un modelo del ambiente para estimar las funciones de valor de acción, sino que se basa únicamente en la recolección de <em>experiencias</em>: secuencias de estados, acciones y recompensas en forma de episodios. En este contexto, suele ser más útil estimar directamente los valores de acción en lugar de los valores de estado, ya que sin conocer la dinámica del entorno, los valores de estado por sí solos no permiten derivar una política.</p>
<p>Una complicación importante de este método es la posibilidad de que ciertos pares estado–acción nunca sean visitados, lo que impide estimar sus valores. La forma más común de mitigar este problema es utilizar políticas estocásticas, que aseguran una probabilidad positiva de seleccionar todas las acciones posibles. Para garantizar esta propiedad, se emplean principalmente dos tipos de métodos.</p>
</div>
</section>
<section id="on-policy-vs-off-policy" class="slide level2">
<h2><em>On policy</em> vs <em>off policy</em></h2>
<div style="font-size: 70%;">
<p>Los métodos on-policy evalúan y mejoran la misma política que utilizan para generar las decisiones, mientras que los off-policy aprenden a mejorar una política distinta de aquella con la que se recopilan los datos.</p>
<p>En los metodos on-policy, la politica suele ser <em>suave</em>, es decir, cumple que <span class="math inline">\(\pi(a|s) &gt; 0\)</span> <span class="math inline">\(\ \forall s \in \boldsymbol{S} \ \forall a \in \boldsymbol{A}\)</span>. De este modo, la política va ajustándose gradualmente hasta converger hacia una política óptima determinista. Estos algoritmos suelen ser más simples conceptualmente que los off-policy y, por ello, se estudian primero.</p>
<p>Por su parte, los métodos off-policy requieren una mayor carga conceptual y notacional. Al basarse en datos generados por otra política, suelen presentar mayor varianza y una convergencia más lenta. Sin embargo, ofrecen mayor flexibilidad y generalidad: los métodos on-policy pueden verse como un caso particular en el que la política de mejora coincide con la política de comportamiento.</p>
</div>
</section>
<section id="diferencias-temporales" class="slide level2">
<h2>Diferencias temporales</h2>
<div style="font-size: 70%;">
<p>Una de las principales desventajas de los métodos de Monte Carlo es que requieren esperar hasta que finalice el episodio completo para actualizar la estimación de la función de valor. Ahi es donde entra el <em>Aprendizaje por Diferencias Temporales(TD)</em>. Aquí es donde entra el Aprendizaje por Diferencias Temporales (TD). Este enfoque permite actualizar las estimaciones en línea, utilizando remuestreo (bootstrap), sin necesidad de esperar al final del episodio.</p>
<p>El objetivo de TD (o TD Target) es la estimación: <span class="math inline">\(E_\pi[R_{t+1}+\gamma G_{t+1}|S_T=s]\)</span> (1.4) con la cual se corrigen las estimaciones previas.</p>
<p>La regla de actualización se expresa como: <span class="math inline">\(Est_{new} = Est_{old} + \alpha [TDTarget - Est_{old}]\)</span></p>
<p>donde <span class="math inline">\(\alpha\)</span> es la tasa de aprendizaje y <span class="math inline">\(\delta_t = [target - old_est]\)</span> el <em>error de diferencia temporal</em></p>
<p>Este error <span class="math inline">\(\delta_t\)</span> es una estimación en el momento t que depende de la recompensa y de los estados siguientes.</p>
</div>
</section>
<section id="algoritmo" class="slide level2">
<h2>Algoritmo</h2>
<div style="font-size: 70%;">
<p>El algoritmo de los metodos TD es esencialmente el mismo para todos.</p>
<ul>
<li>Por cada episodio:
<ul>
<li>Iniciar el estado S.</li>
<li>Elegir una acción A a partir de S siguiendo la política <span class="math inline">\(\pi\)</span>.</li>
<li>Repetir para cada paso del episodio.
<ul>
<li>Ejecutar la acción A, obteniendo una recompensa R y el siguiente estado S’.</li>
<li>Seleccionar una acción A’ en S’ siguiendo la política <span class="math inline">\(\pi\)</span>.</li>
<li>Actualizar la funcion de valor.</li>
<li>Actualizar S <span class="math inline">\(\leftarrow\)</span> S’, A <span class="math inline">\(\leftarrow\)</span> A’.</li>
</ul></li>
<li>Terminar cuando S sea un estado terminal.</li>
</ul></li>
</ul>
<p>La diferencia entre los distintos métodos TD radica en cómo se actualiza la función de valor.</p>
</div>
</section>
<section id="sarsa-y-q-learning" class="slide level2">
<h2>Sarsa y Q-learning</h2>
<div style="font-size: 70%;">
<p>Uno de los métodos TD más conocidos es <strong>SARSA</strong>, cuyo nombre proviene de la secuencia de elementos que intervienen en la actualización dentro del MDP:(S, A, R, S, A). Se trata de un método on-policy, ya que utiliza la acción seleccionada en el nuevo estado S’ para actualizar la función de valor:</p>
<p><span class="math inline">\(Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]\)</span></p>
<p>La convergencia de <strong>SARSA</strong> depende de las características de la política utilizada (por ejemplo, que sea suficientemente exploratoria).</p>
<p>Otro método TD fundamental es Q-learning. A diferencia de SARSA, es un método off-policy, ya que actualiza la función de valor utilizando la acción <em>greedy</em> (aquella que maximize Q), independientemente de la política usada:</p>
<p><span class="math inline">\(Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma \underset{a}{max} Q(S_{t+1},a)-Q(S_t,A_t)]\)</span></p>
<p>Bajo ciertas condiciones sobre la tasa de aprendizaje y la exploración, ambos algoritmos convergen hacia la función de valor óptima.</p>
</div>
</section>
<section id="sarsa-esperado" class="slide level2">
<h2>Sarsa esperado</h2>
<div style="font-size: 70%;">
<p>Al igual que <strong>SARSA</strong>, el <strong>SARSA esperado</strong> es un algoritmo on-policy. La diferencia principal es que, en lugar de usar la acción tomada en el estado S’, utiliza el valor esperado de todas las acciones posibles bajo la política <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math inline">\(Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma E_\pi[Q(S_{t+1},A_{t+1})|S_{t+1}]-Q(S_t,A_t)]\)</span></p>
<p><span class="math inline">\(Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma \underset{a}{\sum}\pi(a|S_{t+1})Q(S_{t+1},a)-Q(S_t,A_t)]\)</span></p>
<p>Aunque es más costoso computacionalmente que <strong>SARSA</strong>, este enfoque elimina la variabilidad introducida por la acción particular <span class="math inline">\(A_{t+1}\)</span>. En la práctica, suele ofrecer un rendimiento ligeramente superior al de <strong>SARSA</strong> cuando se entrena con la misma cantidad de experiencia.</p>
</div>
</section>
<section id="comparación-de-metodos-cliffwalker" class="slide level2">
<h2>Comparación de metodos: Cliffwalker</h2>
<div class="quarto-auto-generated-content">
<div class="footer footer-default">

</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="Introduccion-02-10-2025_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="Introduccion-02-10-2025_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="Introduccion-02-10-2025_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="Introduccion-02-10-2025_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="Introduccion-02-10-2025_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="Introduccion-02-10-2025_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="Introduccion-02-10-2025_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="Introduccion-02-10-2025_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="Introduccion-02-10-2025_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="Introduccion-02-10-2025_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    

    <script>

      // htmlwidgets need to know to resize themselves when slides are shown/hidden.

      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current

      // slide changes (different for each slide format).

      (function () {

        // dispatch for htmlwidgets

        function fireSlideEnter() {

          const event = window.document.createEvent("Event");

          event.initEvent("slideenter", true, true);

          window.document.dispatchEvent(event);

        }

    

        function fireSlideChanged(previousSlide, currentSlide) {

          fireSlideEnter();

    

          // dispatch for shiny

          if (window.jQuery) {

            if (previousSlide) {

              window.jQuery(previousSlide).trigger("hidden");

            }

            if (currentSlide) {

              window.jQuery(currentSlide).trigger("shown");

            }

          }

        }

    

        // hookup for slidy

        if (window.w3c_slidy) {

          window.w3c_slidy.add_observer(function (slide_num) {

            // slide_num starts at position 1

            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);

          });

        }

    

      })();

    </script>

    

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>